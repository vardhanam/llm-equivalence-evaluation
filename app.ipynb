{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = '/home/azureuser/llm_equivalence_evaluation/Accuracy Calculation Dataset - Accuracy Calculation Dataset.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Temporarily set the environment variable for the session\n",
    "os.environ['OPENAI_API_KEY'] = 'MY_API_TOKEN'\n",
    "\n",
    "client = OpenAI()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_image_url(image_url):\n",
    "    \"\"\"\n",
    "    Extracts text from an image URL using the OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        image_url (str): The URL of the image.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted text from the image.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": image_url,\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "def get_prompt(conversation_history, user_response):\n",
    "    \"\"\"\n",
    "    Generates a prompt for the LLM based on the conversation history and user response.\n",
    "\n",
    "    Args:\n",
    "        conversation_history (str): A string representation of the conversation history.\n",
    "        user_response (str): The user's response to the final question.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt for the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"Below is a conversation history, a final question, and user response. Treat the conversation history as context, and check whether the user response is the correct answer to the final question. If it is correct then just return the word EQUIVALENT. If it is incorrect return the word NOT_EQUIVALENT. Do not return any other text.\\n\\n\"\n",
    "\n",
    "    prompt += \"Conversation History:\\n\"\n",
    "\n",
    "    # Convert the string representation of conversation history to a list of dictionaries\n",
    "    conversation_history = ast.literal_eval(conversation_history)\n",
    "\n",
    "    for i in range(len(conversation_history) - 1):\n",
    "        message = conversation_history[i]\n",
    "        if 'user' in message:\n",
    "            user_msg = \"\"\n",
    "            if isinstance(message['user'], list):\n",
    "                for item in message['user']:\n",
    "                    if item['type'] == 'text':\n",
    "                        user_msg += item['text'] + ' '\n",
    "                    elif item['type'] == 'image_url':\n",
    "                        image_url = item['image_url']['url']\n",
    "                        image_text = get_text_from_image_url(image_url)\n",
    "                        user_msg += f\"User provided image: {image_text} \"\n",
    "            else:\n",
    "                user_msg = message['user']\n",
    "            prompt += f\"\\tUser: {user_msg.strip()}\\n\"\n",
    "\n",
    "        if 'bot' in message:\n",
    "            prompt += f\"\\tBot: {message['bot']}\\n\"\n",
    "\n",
    "    final_question = conversation_history[-1]['bot']\n",
    "    prompt += f\"\\nFinal Question: {final_question}\\n\"\n",
    "    prompt += f\"User Response: {user_response}\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Processes the DataFrame by applying the get_prompt function to each row.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame containing conversation history and user responses.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The processed DataFrame with an additional 'Prompt' column.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_prompt(row):\n",
    "        try:\n",
    "            prompt = get_prompt(row['Conversation History'], row['User Response'])\n",
    "            return prompt\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {e}\")\n",
    "            return None\n",
    "\n",
    "    df['Prompt'] = df.apply(generate_prompt, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the prompts to be sent to the LLM\n",
    "process_dataframe(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_llm_response(prompt):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the OpenAI API and retrieves the LLM response.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the OpenAI API.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the LLM response, time taken, and cost.\n",
    "            - llm_response (str): The response generated by the LLM.\n",
    "            - time_taken (float): The time taken (in seconds) to generate the response.\n",
    "            - cost (float): The cost of generating the response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        cost = response.usage.completion_tokens * 0.00003 + response.usage.prompt_tokens * 0.00001\n",
    "        return response.choices[0].message.content, time_taken, cost\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def process_llm_response(row):\n",
    "    \"\"\"\n",
    "    Processes a row of the DataFrame by sending the prompt to the OpenAI API and retrieving the LLM response.\n",
    "\n",
    "    Args:\n",
    "        row (pandas.Series): A row of the DataFrame containing the 'Prompt' column.\n",
    "\n",
    "    Returns:\n",
    "        pandas.Series: A series containing the LLM response, time taken, and cost.\n",
    "    \"\"\"\n",
    "    if pd.notnull(row['Prompt']):\n",
    "        llm_response, time_taken, cost = get_llm_response(row['Prompt'])\n",
    "        return pd.Series([llm_response, time_taken, cost])\n",
    "    else:\n",
    "        return pd.Series([None, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the final results\n",
    "df[['LLM Equivalence Evaluation (Response)', 'Time taken to complete the request', 'Cost in dollars for the request']] = df.apply(process_llm_response, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_equivalence(row):\n",
    "    if row['Human Evaluation'] == 'NOT_EQUIVALENT' and row['LLM Equivalence Evaluation (Response)'] == 'NOT_EQUIVALENT':\n",
    "        return 'True Negative'\n",
    "    elif row['Human Evaluation'] == 'EQUIVALENT' and row['LLM Equivalence Evaluation (Response)'] == 'EQUIVALENT':\n",
    "        return 'True Positive'\n",
    "    else:\n",
    "        return 'Mismatch'\n",
    "\n",
    "df['Evaluation Result'] = df.apply(evaluate_equivalence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'Prompt' column from the updated_df DataFrame\n",
    "df = df.drop('Prompt', axis=1)\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df.to_csv('final_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
